# V3 Design: Closing the Learning Loop

> Date: 2026-02-12
> Status: Approved
> Approach: Greenfield build. V2 codebase is reference material. Existing database and data are preserved — schema changes are additive only.

## Problem Statement

V2 proved the pipeline works end-to-end: news ingestion, LLM strategy, trade execution, outcome tracking. But the learning loop never closes. Attribution is computed and displayed as advisory text that the strategist ignores. The executor re-interprets playbooks from lossy string context. Backfill only runs in certain code paths. Two modules compute signal metrics differently. The system cannot answer its core question — "can agentic trading find an edge?" — because it doesn't enforce what it learns.

V3 is a clean rebuild that makes the learning loop the product. Every component is designed around one principle: decisions flow forward through typed contracts, outcomes flow backward through attribution constraints, and both flows are traceable.

## Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| LLM provider | Claude only (no Ollama) | Haiku classifies 300 headlines for ~$0.02/day. No GPU container, no model management, one auth provider. |
| Embedding filter | None | Pre-filtering saved LLM calls when local inference was free. At Haiku rates, classifying everything costs pennies. Not worth the code. |
| Attribution enforcement | System prompt constraints | Attribution scores injected into strategist system prompt with override-with-justification policy. Cached at $0.50/1M tokens. |
| Playbook format | Structured JSON via `playbook_actions` table | Typed objects from strategist to executor. Decisions trace back to specific playbook actions. |
| Backfill timing | Session start, before all stages | Attribution is always fresh when strategist and executor run. |
| Signal metrics | Single source via `decision_signals` FK | One query pattern for all signal performance. No ad-hoc time-window JOINs. |
| DB layer | Shared connection module, separate query files | One `get_cursor()`, two query namespaces (trading, dashboard). |
| Approach | Greenfield | V2's module boundaries, naming, and test patterns carry forward. The internal wiring is rebuilt. |

## Architecture

### Infrastructure

```
┌──────────────────────────────────────────────────────┐
│                  Docker Compose Stack                  │
├──────────────┬────────────────┬───────────────────────┤
│  PostgreSQL  │   Trading      │   Dashboard           │
│  16          │   Agent        │   Flask :3000          │
│  :5432       │   (Python)     │   (Tailwind+Chart.js)  │
└──────────────┴────────────────┴───────────────────────┘
                       │
         ┌─────────────┼─────────────┐
         ▼             ▼             ▼
   Alpaca Trading  Alpaca Data  Anthropic Claude
   (orders/acct)  (bars/quotes) (Opus + Haiku)
```

Three containers. No GPU. One external LLM provider.

### Session Flow

```
run_session()
  ├─ Stage 0: Refresh learning data
  │   ├─ backfill_outcomes(7d)
  │   ├─ backfill_outcomes(30d)
  │   └─ compute_signal_attribution()
  │
  ├─ Stage 1: News pipeline
  │   ├─ fetch_broad_news(300 headlines)
  │   ├─ classify_with_haiku(batch_size=50)
  │   └─ insert news_signals + macro_signals
  │
  ├─ Stage 2: Strategist (Opus)
  │   ├─ system prompt includes attribution constraints
  │   ├─ manages theses via tools
  │   └─ writes playbook → playbook_actions table
  │
  └─ Stage 3: Executor (Haiku)
      ├─ receives structured ExecutorInput JSON
      ├─ decides per playbook action: execute/adjust/skip
      ├─ flags off-playbook trades explicitly
      └─ logs decisions with playbook_action_id
```

### Data Flow (The Learning Loop)

```
         ┌─────────────────────────────────────────┐
         │                                         │
         ▼                                         │
  News Pipeline ──→ Signals DB                     │
                       │                           │
                       ▼                           │
                 Strategist ──→ Playbook Actions    │
                                    │              │
                                    ▼              │
                              Executor ──→ Decisions + Outcomes
                                                   │
                                                   ▼
                                            Backfill (7d/30d P&L)
                                                   │
                                                   ▼
                                          Attribution Scores
                                                   │
                                                   ▼
                                      Strategist System Prompt ───┘
```

Every arrow is typed. Every link is stored in the database. Every outcome feeds back.

---

## Module Design

### 1. Database Layer (`trading/database/`)

Single connection factory shared by all consumers.

```
trading/database/
    __init__.py          # exports get_connection, get_cursor
    connection.py        # get_connection(), get_cursor() context manager
    trading_db.py        # trading CRUD operations
    dashboard_db.py      # dashboard read queries
```

`connection.py` owns the `DATABASE_URL` env var, the psycopg2 connection, and the `@contextmanager get_cursor()` that yields a `RealDictCursor` with auto-commit/rollback. Everything else imports from here.

`trading_db.py` contains all write operations and trading reads: signals, decisions, theses, playbooks, playbook_actions, positions, orders, snapshots, attribution.

`dashboard_db.py` contains read-only queries optimized for display: aggregations, stats, equity curves, formatted summaries.

### 2. News Pipeline (`trading/news.py`, `trading/classifier.py`, `trading/pipeline.py`)

**`news.py`** — Alpaca News API client. Fetches headlines. Returns `list[NewsItem]` dataclass. Unchanged from V2's interface.

**`classifier.py`** — Classifies headlines via Claude Haiku. Takes a list of headlines, returns structured `ClassificationResult` objects with `news_type`, `ticker_signals[]`, and `macro_signal`.

- Uses `anthropic` SDK directly (no Ollama, no embeddings)
- Batch size of 50 headlines per API call (Haiku handles this comfortably)
- Same output schema as V2: `TickerSignal`, `MacroSignal`, `ClassificationResult` dataclasses
- Falls back to per-headline classification on batch parse failure
- Temperature 0.0 for deterministic classification

**`pipeline.py`** — Orchestrates: fetch → classify → store. Two steps instead of V2's four (no embed, no filter).

```python
@dataclass
class PipelineStats:
    news_fetched: int
    ticker_signals_stored: int
    macro_signals_stored: int
    noise_dropped: int
    errors: list[str]

def run_pipeline(hours=24, limit=300, dry_run=False) -> PipelineStats
```

### 3. Learning System (`trading/backfill.py`, `trading/attribution.py`, `trading/patterns.py`)

**`backfill.py`** — Fetches historical prices from Alpaca, fills in `outcome_7d` and `outcome_30d` on past decisions. Same logic as V2: find decisions needing backfill, get price on target date, compute percentage change.

**`attribution.py`** — The core of the learning loop. Two responsibilities:

1. `compute_signal_attribution()` — JOINs `decision_signals` → `decisions` → `news_signals`/`macro_signals`. Groups by composite category (e.g., `news_signal:earnings`). Computes sample_size, avg_outcome, win_rate. Upserts to `signal_attribution` table.

2. `build_attribution_constraints() -> str` — **New in V3.** Reads `signal_attribution`, formats into constraint block for the strategist system prompt:

```
SIGNAL PERFORMANCE (last 60 days):
  STRONG (>55% win rate): macro_signal:fed (62%, n=23), news_signal:earnings (58%, n=41)
  WEAK (<45% win rate): news_signal:legal (38%, n=12), macro_signal:geopolitical (41%, n=8)
  INSUFFICIENT DATA (<5 samples): news_signal:product (n=3)

CONSTRAINT: Do not create theses primarily based on WEAK signal categories
unless you have a specific reason to override (explain in thesis text).
```

**`patterns.py`** — Analytical queries for the learning loop and dashboard. All signal-level analysis queries through `decision_signals` FK (not time-window JOINs). Functions:

- `analyze_signal_categories(days)` — via `decision_signals` + `signal_attribution`
- `analyze_sentiment_performance(days)` — via `decision_signals` → signal table → sentiment
- `analyze_ticker_performance(days)` — groups decisions by ticker (no signal JOIN needed)
- `analyze_confidence_correlation(days)` — stated confidence vs actual outcomes
- `get_best_performing_signals()` / `get_worst_performing_signals()` — reads `signal_attribution` directly
- `generate_pattern_report(days)` — orchestrates all of the above into text

### 4. Strategist (`trading/ideation_claude.py`, `trading/claude_client.py`, `trading/tools.py`)

**`claude_client.py`** — Claude API wrapper. Provides:
- `get_claude_client()` — from `ANTHROPIC_API_KEY`
- `_call_with_retry()` — exponential backoff for rate limits, server errors, connection errors
- `run_agentic_loop()` — multi-turn tool-use conversation with token tracking
- Prompt caching support (ephemeral `cache_control` on last user message)

Same interface as V2. Reliable, well-tested.

**`tools.py`** — 11 tool definitions + handlers for the strategist's agentic loop:

| Tool | Handler | Notes |
|------|---------|-------|
| `get_market_snapshot` | Fetches sectors, indices, movers, volume | Unchanged |
| `get_portfolio_state` | Positions + account info | Unchanged |
| `get_active_theses` | Active theses, optional ticker filter | Unchanged |
| `create_thesis` | Creates thesis, rejects duplicates | Unchanged |
| `update_thesis` | Dynamic field update | Unchanged |
| `close_thesis` | Sets status + reason | Unchanged |
| `get_news_signals` | Recent ticker news | Unchanged |
| `get_macro_context` | Macro signals by category | Unchanged |
| `get_signal_attribution` | Attribution summary text | Unchanged |
| `get_decision_history` | Recent decisions with outcomes | Unchanged |
| `write_playbook` | **Rewritten** — writes to `playbooks` + `playbook_actions` | See below |

Plus server-side `web_search` tool.

**`write_playbook` changes:** The tool handler now validates and stores structured actions:

- Writes `playbooks` row (outlook, risk_notes, watch_list) — same as V2
- Writes individual `playbook_actions` rows for each priority action
- Validates: non-empty ticker, valid thesis_id if provided, no conflicting actions (buy + sell same ticker), valid confidence, positive max_quantity
- Returns confirmation with action IDs

**`ideation_claude.py`** — Strategist orchestration. Key change: system prompt includes attribution constraints.

```python
def run_strategist_loop(model, max_turns, attribution_constraints: str) -> StrategistResult:
    system_prompt = CLAUDE_STRATEGIST_SYSTEM + "\n\n" + attribution_constraints
    # ... run agentic loop with tools
```

The `run_strategist_session()` function no longer runs backfill/attribution itself — that's handled by Stage 0 in `session.py`. It receives pre-computed attribution constraints as input.

### 5. Executor (`trading/agent.py`, `trading/executor.py`, `trading/trader.py`)

**`agent.py`** — Gets structured decisions from Claude Haiku. **Rebuilt around typed contracts.**

Input contract:
```python
@dataclass
class PlaybookAction:
    id: int
    ticker: str
    action: str                        # buy, sell, hold, watch
    thesis_id: int | None
    reasoning: str
    confidence: str                    # high, medium, low
    max_quantity: Decimal | None
    priority: int

@dataclass
class ExecutorInput:
    playbook_actions: list[PlaybookAction]
    positions: list[dict]
    account: dict                      # cash, buying_power, equity
    attribution_summary: dict          # category → {win_rate_7d, sample_size}
    recent_outcomes: list[dict]        # last 30d decisions with P&L
    market_outlook: str
    risk_notes: str
```

Output contract:
```python
@dataclass
class ExecutorDecision:
    playbook_action_id: int | None     # None = off-playbook
    ticker: str
    action: str                        # buy, sell, hold
    quantity: float
    reasoning: str
    confidence: str
    is_off_playbook: bool
    signal_refs: list[dict]

@dataclass
class ThesisInvalidation:
    thesis_id: int
    reason: str

@dataclass
class AgentResponse:
    decisions: list[ExecutorDecision]
    thesis_invalidations: list[ThesisInvalidation]
    market_summary: str
    risk_assessment: str
```

The executor prompt is simplified: "Here are today's playbook actions. For each, decide: execute as-is, adjust quantity, or skip with justification. You may propose off-playbook trades for urgent situations, but flag them explicitly."

Executor receives `ExecutorInput` serialized as JSON — not prose. No re-parsing, no lossy translation.

**`executor.py`** — Alpaca trade execution. Same interface as V2: `get_trading_client()`, `get_account_info()`, `sync_positions_from_alpaca()`, `sync_orders_from_alpaca()`, `execute_market_order()`, `validate_decision()`.

**`trader.py`** — Trading session orchestrator. Key change: uses `build_executor_input()` instead of `build_trading_context()`.

**`context.py`** — Replaced. Instead of building a prose string, `build_executor_input()` returns an `ExecutorInput` dataclass. Each data element fetched once — no N+1 pattern.

```python
def build_executor_input(account_info: dict, playbook_date: date = None) -> ExecutorInput:
    # One query per data source, assembled into typed dataclass
```

### 6. Session Orchestrator (`trading/session.py`)

Four stages with error isolation:

```python
@dataclass
class SessionResult:
    learning_result: LearningRefreshResult | None
    pipeline_result: PipelineStats | None
    strategist_result: StrategistResult | None
    trading_result: TradingSessionResult | None
    learning_error: str
    pipeline_error: str
    strategist_error: str
    trading_error: str
    duration_seconds: float

def run_session(
    dry_run=False,
    model="claude-opus-4-6",
    executor_model="claude-haiku-4-5-20251001",
    max_turns=25,
    skip_pipeline=False,
    skip_ideation=False,
    pipeline_hours=24,
    pipeline_limit=300,
) -> SessionResult
```

Stage 0 (learning refresh) produces `attribution_constraints: str` that Stage 2 (strategist) consumes. If Stage 0 fails, strategist runs without constraints. If Stage 2 fails, executor runs with the most recent playbook (if one exists for today).

### 7. Market Data (`trading/market_data.py`)

Sector ETF performance, index levels, top movers, unusual volume. Same as V2 — data comes from Alpaca `StockHistoricalDataClient`. Used by the strategist's `get_market_snapshot` tool.

### 8. Learning Loop Orchestrator (`trading/learn.py`)

Standalone learning analysis, separate from the daily session. Can be run independently for deeper analysis.

```python
def run_learning_loop(analysis_days=60, dry_run=False) -> LearningResult
```

Runs: backfill → pattern analysis → attribution. Same stages as session Stage 0, but with the full pattern report.

### 9. Dashboard (`dashboard/`)

Flask app on port 3000 with Tailwind CSS + Chart.js. Server-side rendered.

Routes unchanged from V2. Key change: imports from `trading.database.dashboard_db` instead of local `queries.py`.

No new pages. Playbook compliance rate can be added as a stat card on the existing decisions page when there's enough data.

---

## Database Schema

### Existing Schema (preserved, all data retained)

V3 runs on the existing V2 database. All tables, data, and indexes are kept as-is. The existing schema is defined across:

- `db/init/001_schema.sql` — news_signals, macro_signals, positions, account_snapshots, decisions
- `db/init/002_theses.sql` — theses
- `db/init/005_redesign.sql` — playbooks, decision_signals, signal_attribution, open_orders
- `db/migrations/001_widen_ticker_columns.sql` — VARCHAR(10) → VARCHAR(128)

These files are **not modified**. All existing data (signals, decisions, theses, snapshots, attribution scores) is preserved and continues to be queryable.

### V3 Additive Migration (`db/init/006_v3.sql`)

V3 adds one new table and two new columns. No drops, no renames, no type changes.

```sql
-- Structured playbook actions (normalized from playbooks.priority_actions JSONB)
-- The JSONB column stays on playbooks for backward compat; this table is the
-- primary source for V3's executor input and playbook compliance tracking.
CREATE TABLE playbook_actions (
    id SERIAL PRIMARY KEY,
    playbook_id INT REFERENCES playbooks(id),
    ticker VARCHAR(128) NOT NULL,
    action VARCHAR(10) NOT NULL,        -- buy, sell, hold, watch
    thesis_id INT REFERENCES theses(id),
    reasoning TEXT,
    confidence VARCHAR(10),             -- high, medium, low
    max_quantity DECIMAL,
    priority INT,                       -- 1 = highest
    created_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_playbook_actions_playbook_id ON playbook_actions(playbook_id);

-- Link decisions back to the playbook action that triggered them.
-- NULL for all pre-V3 decisions (they predate structured playbooks).
ALTER TABLE decisions ADD COLUMN playbook_action_id INT REFERENCES playbook_actions(id);
ALTER TABLE decisions ADD COLUMN is_off_playbook BOOLEAN DEFAULT FALSE;

-- Missing index that speeds up attribution queries (decision_signals → decisions JOIN).
-- Existing data benefits from this immediately.
CREATE INDEX IF NOT EXISTS idx_decision_signals_decision_id ON decision_signals(decision_id);
```

### What Existing Data Means to V3

| Existing Data | V3 Treatment |
|---------------|-------------|
| `news_signals` / `macro_signals` | Continues to be written by pipeline, read by strategist tools. No change. |
| `decisions` (pre-V3 rows) | `playbook_action_id` is NULL, `is_off_playbook` is FALSE. Excluded from playbook compliance metrics. Still included in attribution and pattern analysis. |
| `signal_attribution` | Recomputed at every session start from `decision_signals`. Existing rows get overwritten with fresh scores that include all historical decisions. |
| `playbooks` (pre-V3 rows) | `priority_actions` JSONB column still present. V3 strategist writes to both JSONB (for dashboard backward compat) and `playbook_actions` table (for executor). |
| `theses` | Unchanged. Referenced by `playbook_actions.thesis_id`. |
| `decision_signals` | Unchanged. Remains the source of truth for attribution. The new index improves query performance on existing data. |

### Key Relationships

```
Existing (unchanged):
  decisions ←──────────────── decision_signals (decision_id FK)
  decision_signals ──refs──→  news_signals / macro_signals / theses
  decision_signals ──agg──→   signal_attribution (computed)

V3 additions:
  theses ←─────────────────── playbook_actions (thesis_id FK)
  playbooks ←──────────────── playbook_actions (playbook_id FK)
  playbook_actions ←───────── decisions (playbook_action_id FK, NULL for pre-V3)

Feedback loop (V3):
  signal_attribution ──text→  strategist system prompt (constraint block)
```

---

## File Tree

```
algo/
├── docker-compose.yml
├── Dockerfile
├── requirements.txt
├── .env.example
├── run-docker.sh
├── db/
│   └── init/
│       ├── 001_schema.sql          # Existing V2 core tables (unchanged)
│       ├── 002_theses.sql          # Existing V2 theses (unchanged)
│       ├── 005_redesign.sql        # Existing V2 playbooks/signals (unchanged)
│       └── 006_v3.sql              # V3 additive: playbook_actions + decision columns
├── trading/
│   ├── __init__.py
│   ├── database/
│   │   ├── __init__.py             # exports get_connection, get_cursor
│   │   ├── connection.py           # shared DB connection factory
│   │   ├── trading_db.py           # all trading CRUD
│   │   └── dashboard_db.py         # dashboard read queries
│   ├── agent.py                    # executor LLM decisions (Haiku)
│   ├── attribution.py              # signal attribution + constraint builder
│   ├── backfill.py                 # outcome backfill (7d/30d P&L)
│   ├── classifier.py               # news classification (Haiku)
│   ├── claude_client.py            # Claude API wrapper + agentic loop
│   ├── context.py                  # build_executor_input() — typed, not prose
│   ├── executor.py                 # Alpaca trade execution
│   ├── ideation_claude.py          # Claude strategist + ideation
│   ├── learn.py                    # standalone learning loop
│   ├── log_config.py               # logging setup
│   ├── market_data.py              # sector/index/mover data
│   ├── news.py                     # Alpaca news fetching
│   ├── patterns.py                 # pattern analysis (FK-based queries)
│   ├── pipeline.py                 # news pipeline: fetch → classify → store
│   ├── session.py                  # 4-stage daily session orchestrator
│   ├── tools.py                    # strategist tool definitions + handlers
│   └── trader.py                   # trading session executor
├── dashboard/
│   ├── Dockerfile
│   ├── requirements.txt
│   ├── app.py                      # Flask routes
│   └── templates/
│       ├── base.html
│       ├── portfolio.html
│       ├── playbook.html
│       ├── signals.html
│       ├── theses.html
│       ├── decisions.html
│       ├── attribution.html
│       └── performance.html
├── tests/
│   ├── conftest.py
│   ├── test_agent.py
│   ├── test_attribution.py
│   ├── test_backfill.py
│   ├── test_classifier.py
│   ├── test_claude_client.py
│   ├── test_context.py
│   ├── test_dashboard.py
│   ├── test_db.py
│   ├── test_executor.py
│   ├── test_full_cycle.py
│   ├── test_ideation_claude.py
│   ├── test_learn.py
│   ├── test_market_data.py
│   ├── test_news.py
│   ├── test_patterns.py
│   ├── test_pipeline.py
│   ├── test_session.py
│   ├── test_tools.py
│   └── test_trader.py
├── docs/
│   └── plans/
│       └── 2026-02-12-v3-design.md
└── logs/
```

### What's Gone (vs V2)

| Removed | Why |
|---------|-----|
| `trading/ollama.py` | No local LLM |
| `trading/filter.py` | No embedding filter |
| `trading/ideation.py` | Legacy Ollama ideation |
| `trading/main.py` | Connectivity check — not needed in V3 |
| `scripts/setup-ollama.sh` | No Ollama |
| `dashboard/queries.py` | Moved to `trading/database/dashboard_db.py` |
| `trading/db.py` | Split into `trading/database/` package |
| `tests/test_ollama.py` | Module removed |
| `tests/test_filter.py` | Module removed |
| `tests/test_ideation.py` | Module removed |
| `tests/test_model_integration.py` | Was Ollama integration tests |
| `tests/test_db_redesign.py` | Merged into `test_db.py` |

---

## Out of Scope

- **No ORM** — Raw SQL + psycopg2. ~40 queries, not worth the abstraction.
- **No connection pooling** — Sequential stage execution. Shared module makes it easy to add later.
- **No dashboard auth** — Local tool.
- **No real-time dashboard** — SSR. Checked once a day.
- **No automated scheduling** — Cron or manual.
- **No new dashboard pages** — Playbook compliance as a stat card, not a page.
- **No multi-account** — Single Alpaca account, single strategy.

---

## V2 Reference

The V2 architecture document (`V3_ARCHITECTURE.md` at repo root) contains exhaustive documentation of every V2 module, function, query, and test. Use it as reference for:

- Alpaca API integration patterns (executor, backfill, market_data, news)
- Claude API retry logic and agentic loop design (claude_client)
- Classification prompt templates and output schemas (classifier)
- Test patterns, fixtures, and factory functions (conftest)
- Dashboard routes and template structure
- All SQL query patterns

V2 code that carries forward largely unchanged: `news.py`, `executor.py`, `backfill.py`, `claude_client.py`, `market_data.py`, `log_config.py`, `learn.py`, dashboard templates.
